---
title: "Rachel Jakielski CS1675 Final Project"
output: html_document
---

***PART FOUR: Binary Classification***

Note: Long iterations printed for GBTs 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr )
library(caret)
```


### Reading in the data: 

Base Features: 

```{r, base_df}
df <- readr::read_csv("cs_1675_fall2021_finalproject.csv", col_names = TRUE)%>%
  mutate(y = boot::logit(output))%>%
              mutate(outcome = ifelse(output < 0.33, 'event', 'non_event'),
              outcome = factor(outcome, levels = c("event", "non_event"))) %>% 
              subset(select = c(-output, -y)) %>% glimpse()

```

Expanded Features: 


```{r, derived_df}
derived_df <- df %>% 
              mutate(x5 = 1 - (x1 + x2 + x3 + x4),
                     w = x2 / (x3 + x4),
                     z = (x1 + x2) / (x5 + x4),
                     t = v1 * v2) %>% 
              glimpse()
```

# Train By Maximizing Accuracy: 


```{r, make_caret_info_setup}
my_ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3)

acc <- "Accuracy"
```


## Logistic Regression: 

Additive features using the base feature set: 

```{r}
set.seed(825)
fit_lm_base_acc <- train(outcome ~ (.),
                  data = df,
                  method = "glm",
                  metric = acc,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)
```


Additive features using the “expanded feature” set:
 

```{r}
set.seed(825)
fit_lm_expanded_acc <- train(outcome ~ (x1+x2+x5+w+z),
                  data = derived_df,
                  method = "glm",
                  metric = acc,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)
```

Top ranked linear model from Part ii): 

```{r}
set.seed(825)
fit_lm_top_ranked_acc <- train(outcome ~ splines::ns(x1,df=4) * (w+z+x2+x3),
                  data = derived_df,
                  method = "glm",
                  metric = acc,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

```

Another linear model from Part ii): 

```{r}
set.seed(825)
fit_lm_choice_acc <- train(outcome ~ x1*(.-x4-v1-w),
                  data = derived_df,
                  method = "glm",
                  metric = acc,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

```

## Elastic Net: 

Interact the categorical variable with all pair-wise interactions of the continuous features: 

```{r, fit_mod_03}
set.seed(825)
fit_enet_cc_acc <- train(outcome ~ m*(I(x1)^2+I(x2)^2+I(x5)^2+I(w)^2+I(z)^2),
                    data = derived_df,
                    method = "glmnet",
                    metric = acc,
                    preProcess = c("center", "scale"),
                    trControl = my_ctrl)

```

The most complex model tried in Part ii): 

```{r, fit_complex}
set.seed(825)
fit_enet_complex_acc <- train(outcome ~ splines::ns(x1,df=4) * (w+z+t+x2+x4),
                    data = derived_df,
                    method = "glmnet",
                    metric = acc,
                    preProcess = c("center", "scale"),
                    trControl = my_ctrl)

```

## Neural Network: 

Base Features: 

```{r, nnet_base}
set.seed(825)
fit_nnet_base_acc <- train(outcome ~ (.),
                    data = df,
                    method = "nnet",
                    metric = acc,
                    preProcess = c("center", "scale"),
                    trControl = my_ctrl,
                    trace = FALSE)

```


Expanded Features: 

```{r, nnet_expanded}
set.seed(825)
fit_nnet_expanded_acc <- train(outcome ~ (x1+x2+x5+w+z),
                    data = derived_df,
                    method = "nnet",
                    metric = acc,
                    preProcess = c("center", "scale"),
                    trControl = my_ctrl,
                    trace = FALSE)

```


## Random Forest: 

Base Features: 

```{r}
set.seed(825)
fit_rf_base_acc <- train(outcome ~ (.),
                data = df,
                method = "rf",
                metric = acc,
                trControl = my_ctrl,
                importance = TRUE)

```

Expanded Features: 

```{r}
set.seed(825)
fit_rf_expanded_acc <- train(outcome ~ (x1+x2+x5+w+z),
                data = derived_df,
                method = "rf",
                metric = acc,
                trControl = my_ctrl,
                importance = TRUE)

```

## GBT: 

Base Features: 


```{r, warning=FALSE, message=FALSE}
set.seed(825)
fit_xgb_base_acc <- train(outcome ~ (.),
                 data = df,
                 method = "xgbTree",
                 metric = acc,
                 trControl = my_ctrl)

```

Try refining a tuning grid based off fit_xgb_base: 

```{r}

GBT_base_grid_acc <- expand.grid(nrounds = seq(100, 700, by = 100),
                        max_depth = c(3, 4, 5),
                        eta = c(0.5*fit_xgb_base_acc$bestTune$eta, fit_xgb_base_acc$bestTune$eta),
                        gamma = fit_xgb_base_acc$bestTune$gamma,
                        colsample_bytree = fit_xgb_base_acc$bestTune$colsample_bytree,
                        min_child_weight = fit_xgb_base_acc$bestTune$min_child_weight,
                        subsample = fit_xgb_base_acc$bestTune$subsample)

```

```{r,  warning=FALSE, message=FALSE}
set.seed(825)
fit_xgb_base_tuned_acc <- train(outcome ~ (.),
                 data = df,
                 method = "xgbTree",
                 tuneGrid = GBT_base_grid_acc,
                 metric = acc,
                 trControl = my_ctrl)

```

Expanded Features: 

```{r, warning=FALSE, message=FALSE}
set.seed(825)
fit_xgb_expanded_acc <- train(outcome ~ (x1+x2+x5+w+z),
                 data = derived_df,
                 method = "xgbTree",
                 metric = acc,
                 trControl = my_ctrl)
```

Try refining a tuning grid based off fit_xgb_expanded: 

```{r,}

GBT_expanded_grid_acc <-expand.grid(nrounds = seq(100, 700, by = 100),
                        max_depth = c(3, 4, 5),
                        eta = c(0.5*fit_xgb_expanded_acc$bestTune$eta, fit_xgb_expanded_acc$bestTune$eta),
                        gamma = fit_xgb_expanded_acc$bestTune$gamma,
                        colsample_bytree = fit_xgb_expanded_acc$bestTune$colsample_bytree,
                        min_child_weight = fit_xgb_expanded_acc$bestTune$min_child_weight,
                        subsample = fit_xgb_expanded_acc$bestTune$subsample)

```

```{r, warning=FALSE, message=FALSE}

set.seed(825)
fit_xgb_expanded_tuned_acc <- train(outcome ~ (x1+x2+x5+w+z),
                 data = derived_df,
                 method = "xgbTree",
                 tuneGrid = GBT_expanded_grid_acc,
                 metric = acc,
                 trControl = my_ctrl)

```

## Additional Methods:

Support Vector Machine (SVM): 

```{r, fit_SVM}
set.seed(825)
fit_svm_acc <- train(outcome ~ (x1+x2+x5+w+z),
                 data = derived_df,
                 method = "svmRadial",
                 metric = acc,
                 preProcess = c("center", "scale"),
                 trControl = my_ctrl)

```


Deep Nerual Network (DNN): 

```{r, fit_DNN}
DNNgrid <- expand.grid(layer1 = 1:3,
                     layer2 = 0, layer3 = 0,
                     hidden_dropout = c(0, .1), 
                     visible_dropout = 0)
set.seed(825)
fit_DNN_acc <- train(outcome ~ (x1+x2+x5+w+z),
                 data = derived_df,
                 method = "dnn",
                 metric = acc,
                 tuneGrid = DNNgrid,
                 preProcess = c("center", "scale"),
                 trControl = my_ctrl)

```

## Model Analysis: 

```{r, compile_resample_results}


my_ACC_results <- resamples(list(LINEAR_BASE_ACC = fit_lm_base_acc,
                             LINEAR_EXPANDED_ACC = fit_lm_expanded_acc,
                             LINEAR_TOP_ACC = fit_lm_top_ranked_acc,
                             LINEAR_CHOICE_ACC = fit_lm_choice_acc,
                             ENET_PAIRWISE_ACC = fit_enet_cc_acc,
                             ENET_COMPLEX_ACC = fit_enet_complex_acc,
                             NNET_BASE_ACC = fit_nnet_base_acc, 
                             NNET_EXPANDED_ACC = fit_nnet_expanded_acc, 
                             RF_BASE_ACC = fit_rf_base_acc, 
                             RF_EXPANDED_ACC = fit_rf_expanded_acc, 
                             XGB_EXPANDED_ACC = fit_xgb_expanded_acc,
                             XGB_EXPANDED_TUNED_ACC = fit_xgb_expanded_tuned_acc, 
                             XGB_BASE_ACC = fit_xgb_base_acc, 
                             XGB_BASE_TUNED_ACC = fit_xgb_base_tuned_acc, 
                             SVM_ACC = fit_svm_acc, 
                             DNN_ACC= fit_DNN_acc))

```

```{r}
summary(my_ACC_results, metric = 'Accuracy')
```
```{r}
dotplot(my_ACC_results, metric = 'Accuracy')
```

Accuracy of the top 3 models: 

```{r, top_performer1}
confusionMatrix(fit_xgb_expanded_acc)

confusionMatrix(fit_xgb_expanded_tuned_acc)

confusionMatrix(fit_rf_expanded_acc)
```

The best performing model according to accuracy is the gradient boosted tree, using the expanded data set. 

Saving Models:

```{r, Save_Models}
fit_rf_expanded_acc %>% readr::write_rds("RF_EXPANDED_ACC.rds")
fit_xgb_expanded_acc %>% readr::write_rds("XGB_EXPANDED_ACC.rds")
fit_xgb_expanded_tuned_acc %>% readr::write_rds("XGB_EXPANDED_TUNED_ACC.rds")
fit_svm_acc %>% readr::write_rds("SVM_ACC.rds")
```